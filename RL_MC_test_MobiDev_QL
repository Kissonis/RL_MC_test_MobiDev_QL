{"cells":[{"cell_type":"markdown","metadata":{"id":"Hmbm5Dv1TOOe"},"source":["**Connect to the Google Drive**"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18439,"status":"ok","timestamp":1681719391164,"user":{"displayName":"Сергій Ісаков","userId":"08735013337723685791"},"user_tz":-180},"id":"AvmY8RqJnOLP","outputId":"3f78b77b-3906-4d85-b520-b5c7ad7a8472"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Connect to the Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","root = \"/content/drive/MyDrive/Reinforcement_Learning/MountainCar/QL/\""]},{"cell_type":"markdown","source":["**Q-Learning using OpenAI gym MountainCar enviornment**"],"metadata":{"id":"F3zTYJiwTNCV"}},{"cell_type":"code","source":["\n","import numpy as np\n","import cv2\n","import gym\n","from gym import wrappers\n","\n","n_states = 40\n","iter_max = 10000\n","\n","initial_lr = 1.0 # Learning rate\n","min_lr = 0.003\n","gamma = 1.0\n","t_max = 1000#10000\n","eps = 0.02\n","\n","env_name = 'MountainCar-v0'\n","\n","def run_episode(env, policy=None, render=False):\n","    obs = env.reset()\n","    total_reward = 0\n","    step_idx = 0\n","    if render:\n","            fourcc = cv2.VideoWriter_fourcc(*'XVID')\n","            out = cv2.VideoWriter(root + 'video_after_training.avi', fourcc, 30.0, (600, 400))\n","    for _ in range(t_max):\n","        if render:\n","            frame = env.render(mode='rgb_array')\n","            out.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n","\n","        if policy is None:\n","            action = env.action_space.sample()\n","        else:\n","            a,b = obs_to_state(env, obs)\n","            action = policy[a][b]\n","        obs, reward, done, _ = env.step(action)\n","        total_reward += gamma ** step_idx * reward\n","        step_idx += 1\n","        if done:\n","            break\n","    if render: out.release()\n","    return total_reward\n","\n","def obs_to_state(env, obs):\n","    \"\"\" Maps an observation to state \"\"\"\n","    env_low = env.observation_space.low\n","    env_high = env.observation_space.high\n","    env_dx = (env_high - env_low) / n_states\n","    a = int((obs[0] - env_low[0])/env_dx[0])\n","    b = int((obs[1] - env_low[1])/env_dx[1])\n","    return a, b\n"],"metadata":{"id":"1z-75mmPTA4J","executionInfo":{"status":"ok","timestamp":1681722987419,"user_tz":-180,"elapsed":275,"user":{"displayName":"Сергій Ісаков","userId":"08735013337723685791"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["def train():    \n","    env = gym.make(env_name)\n","    env.seed(42)\n","    np.random.seed(42)\n","    print ('----- using Q Learning -----')\n","    q_table = np.zeros((n_states, n_states, 3))\n","    for i in range(iter_max):\n","        obs = env.reset()\n","        total_reward = 0\n","        ## eta: learning rate is decreased at each step\n","        eta = max(min_lr, initial_lr * (0.85 ** (i//100)))\n","        for j in range(t_max):\n","            a, b = obs_to_state(env, obs)\n","            if np.random.uniform(0, 1) < eps:\n","                action = np.random.choice(env.action_space.n)\n","            else:\n","                logits = q_table[a][b]\n","                logits_exp = np.exp(logits)\n","                probs = logits_exp / np.sum(logits_exp)\n","                action = np.random.choice(env.action_space.n, p=probs)\n","            obs, reward, done, _ = env.step(action)\n","            total_reward += reward\n","            # update q table\n","            a_, b_ = obs_to_state(env, obs)\n","            q_table[a][b][action] = q_table[a][b][action] + eta * (reward + gamma *  np.max(q_table[a_][b_]) - q_table[a][b][action])\n","            if done:\n","                break\n","        if i % 1000 == 0:\n","            print('Iteration #%d -- Total reward = %d.' %(i+1, total_reward))\n","    solution_policy = np.argmax(q_table, axis=2)\n","    solution_policy_scores = [run_episode(env, solution_policy, False) for _ in range(100)]\n","    print(\"Average score of solution = \", np.mean(solution_policy_scores))\n","    return solution_policy\n","    "],"metadata":{"id":"bLzmH893kYCU","executionInfo":{"status":"ok","timestamp":1681722992285,"user_tz":-180,"elapsed":263,"user":{"displayName":"Сергій Ісаков","userId":"08735013337723685791"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["solution_policy = train()\n","env = gym.make(env_name)\n","_ = run_episode(env, policy=solution_policy, render=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"21qaQhbBqKk_","executionInfo":{"status":"ok","timestamp":1681723259190,"user_tz":-180,"elapsed":263589,"user":{"displayName":"Сергій Ісаков","userId":"08735013337723685791"}},"outputId":"63fa2727-2f76-4783-92aa-bc4dcb9da909"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["----- using Q Learning -----\n","Iteration #1 -- Total reward = -200.\n","Iteration #1001 -- Total reward = -200.\n","Iteration #2001 -- Total reward = -200.\n","Iteration #3001 -- Total reward = -200.\n","Iteration #4001 -- Total reward = -200.\n","Iteration #5001 -- Total reward = -200.\n","Iteration #6001 -- Total reward = -200.\n","Iteration #7001 -- Total reward = -200.\n","Iteration #8001 -- Total reward = -200.\n","Iteration #9001 -- Total reward = -200.\n","Average score of solution =  -136.76\n"]},{"output_type":"execute_result","data":{"text/plain":["-85.0"]},"metadata":{},"execution_count":22}]}],"metadata":{"colab":{"provenance":[{"file_id":"https://gist.github.com/cwkx/f307bf38aa50bf692e42448111ce6979#file-rl-assignment-ipynb","timestamp":1676284385148}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}